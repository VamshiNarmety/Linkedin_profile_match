{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uslGtr5C8fEd",
        "outputId": "a9db96ff-b981-4dd8-bfa6-c4458f2598fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting flair\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from keybert) (3.4.1)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.37.36-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.30.2)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.3.2)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.6.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.8.2)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/pytorch-revgrad/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.51.1)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.38.0,>=1.37.36 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.37.36-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.11.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.4)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.36->boto3>=1.20.27->flair) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.5.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.37.36-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_smaller_training_vocab-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.37.36-py3-none-any.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m111.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading s3transfer-0.11.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=1d180006c848b6bc2a4170f2294da68c94aec3b460072ce974ff526ac1f84595\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=d438a1fa09b9007ac88b43dbdffb045898e65cbccd6abb827b715e36eccb135a\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/8a/eb/d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=634827909dfd292644c61b22d4e24c0cfd83ca2019dc122cc1a365d70e315736\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=e4c2b48185de22ea1ee5fdd3425404164f509f1d5980ee92762dfb3dd7d95bd3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=1af0b9cb172b34a11a6b0f82d33834d36eec8cf6612bca97374f0e4a4fbb669d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=301a8410bb6e49e6a701eab822859f2a3cbbdeedbac797963da7401df23fbc06\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, wikipedia-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, bioc, s3transfer, nvidia-cusolver-cu12, mpld3, boto3, pytorch-revgrad, keybert, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bioc-2.1 boto3-1.37.36 botocore-1.37.36 conllu-4.5.3 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 keybert-0.9.0 langdetect-1.0.9 mpld3-0.5.10 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.11.5 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.1 wikipedia-api-0.8.1\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.0.1)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.13.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=aac37eed2909a9279a42b446d2d61640c13b18aa1034525e32887b5a1532bea9\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.1 gunicorn-23.0.0 lz4-4.4.4 mtcnn-1.0.0 retina-face-0.0.17\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsm6 is already the newest version (2:1.2.3-1build2).\n",
            "libxext6 is already the newest version (2:1.3.4-1build1).\n",
            "libxrender-dev is already the newest version (1:0.9.10-1build4).\n",
            "libxrender-dev set to manually installed.\n",
            "libglib2.0-0 is already the newest version (2.72.4-0ubuntu2.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.1.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (3.5.0)\n",
            "Collecting locate<2.0.0,>=1.1.1 (from wordfreq)\n",
            "  Downloading locate-1.1.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (1.1.0)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.11/dist-packages (from wordfreq) (2024.11.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.2.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from marisa-trie>=1.1.0->language-data>=1.2->langcodes>=3.0->wordfreq) (75.2.0)\n",
            "Downloading wordfreq-3.1.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading locate-1.1.1-py3-none-any.whl (5.4 kB)\n",
            "Installing collected packages: locate, wordfreq\n",
            "Successfully installed locate-1.1.1 wordfreq-3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install keybert flair\n",
        "!pip install sentence-transformers\n",
        "!pip install deepface\n",
        "!apt install libglib2.0-0 libsm6 libxext6 libxrender-dev -y\n",
        "!pip install wordfreq\n",
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from keybert import KeyBERT\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "import itertools\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional, Dict, Tuple\n",
        "from pydantic import BaseModel\n",
        "import time\n",
        "from typing_extensions import Optional, TypedDict\n",
        "import requests\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, List\n",
        "import difflib\n",
        "from deepface import DeepFace\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from wordfreq import word_frequency\n",
        "import spacy"
      ],
      "metadata": {
        "id": "2Dijkrg68grI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dataset.json\",\"r\") as f:\n",
        "    data=json.load(f)\n",
        "\n",
        "\n",
        "\"\"\"### Queries Code\"\"\"\n",
        "\n",
        "pipeline1_data=[]\n",
        "pipeline2_data=[]\n",
        "pipeline3_data=[]\n",
        "\n",
        "def is_valid(field):\n",
        "    return field is not None and field != \"\"\n",
        "\n",
        "for entry in data:\n",
        "    if is_valid(entry.get(\"intro\")):\n",
        "        pipeline1_data.append(entry)\n",
        "    else:\n",
        "        if not is_valid(entry.get(\"company_industry\")) and not is_valid(entry.get(\"company_size\")):\n",
        "            pipeline2_data.append(entry)\n",
        "        else:\n",
        "            pipeline3_data.append(entry)\n",
        "\n",
        "with open(\"pipe_1.json\", \"w\") as f:\n",
        "    json.dump(pipeline1_data, f, indent=4)\n",
        "\n",
        "with open(\"pipe_2.json\", \"w\") as f:\n",
        "    json.dump(pipeline2_data, f, indent=4)\n",
        "\n",
        "with open(\"pipe_3.json\", \"w\") as f:\n",
        "    json.dump(pipeline3_data, f, indent=4)\n",
        "\n",
        "\n",
        "# print(\"Pipeline 1:\")\n",
        "# print(json.dumps(pipeline1_data, indent=4, ensure_ascii=False))\n",
        "# print(\"Pipeline 2:\")\n",
        "# print(json.dumps(pipeline2_data, indent=4, ensure_ascii=False))\n",
        "# print(\"Pipeline 3:\")\n",
        "# print(json.dumps(pipeline3_data, indent=4, ensure_ascii=False))\n",
        "\n",
        "\"\"\"### Queries Code\"\"\"\n",
        "\n",
        "##Common functions for all pipelines\n",
        "\n",
        "# Load models\n",
        "kw_model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
        "ner_tagger = SequenceTagger.load(\"ner\")\n",
        "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_name(name):\n",
        "    if not isinstance(name, str):\n",
        "        return \"\"\n",
        "    name = re.sub(r\"\\(.*?\\)\", \"\", name).strip()\n",
        "    name = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", name)\n",
        "    return name\n",
        "\n",
        "def extract_words_from_url(text):\n",
        "    urls = re.findall(r'https?://[^\\s\\)]+', text)\n",
        "    keywords = []\n",
        "\n",
        "    for url in urls:\n",
        "        url = url.split('?')[0]  # Remove query parameters like ?utm_source=revgenius\n",
        "        domain_match = re.search(r'https?://(?:www\\.)?([a-zA-Z0-9\\-]+)\\.', url)\n",
        "        if domain_match:\n",
        "            keywords.append(domain_match.group(1).lower())\n",
        "\n",
        "    return list(dict.fromkeys(keywords))\n",
        "\n",
        "def is_uncommon(word):\n",
        "    return word_frequency(word.lower(), 'en') < 1e-5\n",
        "\n",
        "def extract_uncommon_words(text):\n",
        "    words = re.findall(r'\\b[A-Za-z0-9\\-]{3,}\\b', text.lower())\n",
        "    return list(dict.fromkeys([w for w in words if is_uncommon(w)]))\n",
        "\n",
        "def extract_entities(text):\n",
        "    doc = spacy_nlp(text)\n",
        "    return list({ent.text for ent in doc.ents if ent.label_ in {\"ORG\", \"PRODUCT\"}})\n",
        "\n",
        "def extract_keywords(text, max_keywords=5):\n",
        "    keywords = kw_model.extract_keywords(text, top_n=max_keywords, stop_words=\"english\")\n",
        "    return [kw[0] for kw in keywords]\n",
        "\n",
        "def extract_from_intro(text):\n",
        "    uncommon_words = extract_uncommon_words(text)\n",
        "    ner_entities = extract_entities(text)\n",
        "    keywords = extract_keywords(text)\n",
        "\n",
        "    combined = list(dict.fromkeys(uncommon_words + ner_entities + keywords))\n",
        "    return [w for w in combined if len(w.split()) <= 4]\n",
        "\n",
        "def remove_designations(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    titles = [\n",
        "        \"founder\", \"cofounder\", \"co-founder\", \"ceo\", \"cto\", \"cfo\", \"coo\",\n",
        "        \"chief executive officer\", \"chief technology officer\", \"chief financial officer\",\n",
        "        \"chief operating officer\", \"director\", \"vp\", \"president\", \"partner\", \"owner\",\n",
        "        \"principal\", \"manager\", \"lead\", \"head\", \"entrepreneur\", \"investor\", \"consultant\",\n",
        "        \"advisor\", \"angel investor\", \"freelancer\", \"self-employed\"\n",
        "    ]\n",
        "\n",
        "    text = re.sub(r'[-/|&@]', ' ', text)\n",
        "    for prefix in [\"ex-\", \"ex \"]:\n",
        "        for title in titles:\n",
        "            text = re.sub(rf\"\\b{prefix}{re.escape(title)}\\b\", \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    pattern = r'\\b(?:' + '|'.join(re.escape(title) for title in titles) + r')\\b'\n",
        "    text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return re.sub(r'\\s{2,}', ' ', text).strip()\n",
        "\n",
        "def generate_query(person):\n",
        "    name = clean_name(person.get(\"name\", \"\"))\n",
        "    intro_raw = str(person.get(\"intro\", \"\") or \"\")\n",
        "    intro_clean = remove_designations(intro_raw)\n",
        "\n",
        "    # Check for URLs in either raw or cleaned intro\n",
        "    if \"http\" in intro_raw or \"http\" in intro_clean:\n",
        "        keywords = extract_words_from_url(intro_raw + \" \" + intro_clean)\n",
        "    else:\n",
        "        keywords = extract_from_intro(intro_clean)\n",
        "\n",
        "    keywords = [k for k in keywords if k.lower() not in name.lower()]\n",
        "\n",
        "    # De-duplicate case-insensitively while keeping first appearance\n",
        "    seen = set()\n",
        "    unique_keywords = []\n",
        "    for k in keywords:\n",
        "        lower_k = k.lower()\n",
        "        if lower_k not in seen:\n",
        "            seen.add(lower_k)\n",
        "            unique_keywords.append(k)\n",
        "\n",
        "    final_keywords = unique_keywords[:2]\n",
        "    return \" \".join([name] + final_keywords)\n",
        "\n",
        "def process_people(data):\n",
        "    for person in data:\n",
        "        person[\"query\"] = generate_query(person)\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"dataset.json\"\n",
        "    output_file = \"pipe1_processed.json\"\n",
        "\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed = process_people(data)\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(processed, f, indent=4)\n",
        "\n",
        "    print(\"All entries processed and saved to\", output_file)\n",
        "\n",
        "import json\n",
        "import time\n",
        "from typing_extensions import Optional, TypedDict\n",
        "import requests\n",
        "\n",
        "class GoogleCustomSearchResponse(TypedDict):\n",
        "    link: str\n",
        "    title: Optional[str]\n",
        "    snippet: Optional[str]\n",
        "\n",
        "class GoogleCustomSearch:\n",
        "    def __init__(self):\n",
        "        self.api_key = \"AIzaSyBhoMDnst3_JoK5TziGClgS27bhX0dQNFk\"\n",
        "        self.search_engine_id = \"c307652d3fbc44ec8\"\n",
        "        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "    def _search(self, query: str, num=10, start=1) -> Optional[dict]:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"key\": self.api_key,\n",
        "            \"cx\": self.search_engine_id,\n",
        "            \"num\": num,\n",
        "            \"start\": start,\n",
        "        }\n",
        "        response = requests.get(self.base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Google Search Failed: {response.status_code} {response.text}\")\n",
        "            return None\n",
        "\n",
        "    def _parse_results(self, response) -> list[GoogleCustomSearchResponse]:\n",
        "        if not response or \"items\" not in response:\n",
        "            return []\n",
        "        results = []\n",
        "        for item in response[\"items\"]:\n",
        "            link = item.get(\"link\", None)\n",
        "            if link:\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"title\": item.get(\"title\", \"\"),\n",
        "                        \"link\": link,\n",
        "                        \"snippet\": item.get(\"snippet\", \"\"),\n",
        "                    }\n",
        "                )\n",
        "        return results\n",
        "\n",
        "    def search(self, query: str, num=5, start=1) -> list[GoogleCustomSearchResponse]:\n",
        "        response = self._search(query, num, start)\n",
        "        if response:\n",
        "            return self._parse_results(response)\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    # Load personas from processed.json\n",
        "    with open('pipe1_processed.json', 'r', encoding='utf-8') as f:\n",
        "        personas = json.load(f)\n",
        "\n",
        "    searcher = GoogleCustomSearch()\n",
        "    results = {}\n",
        "\n",
        "    for persona in personas:\n",
        "        name = persona.get(\"name\", \"Unknown\")\n",
        "        query = persona.get(\"query\")\n",
        "        print(f\"Searching for: {name} | Query: {query}\")\n",
        "        if not query:\n",
        "            results[name] = []\n",
        "            continue\n",
        "        search_results = searcher.search(f'site:linkedin.com/in {query}', num=5)\n",
        "        results[name] = search_results\n",
        "        time.sleep(1)  # Be polite to the API\n",
        "\n",
        "    # Save results to a new JSON file\n",
        "    with open('pipe1_urls.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"Done! Results saved to pipe1_urls.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "import json\n",
        "import itertools\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Optional, Dict, Tuple\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# --- Data Models ---\n",
        "class ProfileData(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    profile_url: Optional[str] = None\n",
        "    profile_image: Optional[str] = None\n",
        "    about: Optional[str] = None\n",
        "\n",
        "# --- Rotating User Agents ---\n",
        "user_agents = [\n",
        "    \"Slackbot-LinkExpanding 1.0 (+https://api.slack.com/robots)\",\n",
        "    \"LinkedInBot/1.0\",\n",
        "    \"Twitterbot/1.0\",\n",
        "    \"facebookexternalhit/1.1\",\n",
        "    \"WhatsApp/2.0\",\n",
        "    \"Googlebot/2.1 (+http://www.google.com/bot.html)\",\n",
        "]\n",
        "user_agent_cycle = itertools.cycle(user_agents)\n",
        "\n",
        "def mimic_bot_headers() -> str:\n",
        "    \"\"\"Mimic bot headers by cycling through user agents\"\"\"\n",
        "    return next(user_agent_cycle)\n",
        "\n",
        "# --- LinkedIn Scraper ---\n",
        "class LinkedInProvider:\n",
        "    \"\"\"Get basic data (name, image, about) from LinkedIn URL using web scraping\"\"\"\n",
        "\n",
        "    def _fetch_data(self, url: str) -> Optional[str]:\n",
        "        retry_count = 3\n",
        "        for attempt in range(retry_count):\n",
        "            user_agent = mimic_bot_headers()\n",
        "            headers = {\"User-Agent\": user_agent}\n",
        "            proxies = {\n",
        "                \"https\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "                \"http\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, proxies=proxies, timeout=10)\n",
        "                if response.status_code == 200:\n",
        "                    html = response.text\n",
        "                    with open(\"scraped_debug.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "                        f.write(html)\n",
        "                    return html\n",
        "            except Exception as e:\n",
        "                print(f\"Request error for {url}: {e}\")\n",
        "        print(f\"❌ Failed to fetch LinkedIn URL after {retry_count} attempts: {url}\")\n",
        "        return None\n",
        "\n",
        "    def extract_basic_profile_data(self, html: str) -> ProfileData:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        data = ProfileData()\n",
        "\n",
        "        # Extract name\n",
        "        name_tag = soup.find('h1', class_=lambda x: x and 'top-card-layout__title' in x)\n",
        "        data.name = name_tag.get_text(strip=True) if name_tag else None\n",
        "\n",
        "        # Extract canonical LinkedIn URL\n",
        "        canonical = soup.find('link', rel=\"canonical\")\n",
        "        data.profile_url = canonical['href'] if canonical and canonical.get('href') else None\n",
        "\n",
        "        # Extract profile image\n",
        "        if data.name:\n",
        "            profile_img = soup.find('img', alt=data.name)\n",
        "            if profile_img:\n",
        "                data.profile_image = profile_img.get('data-delayed-url') or profile_img.get('src')\n",
        "\n",
        "        # Extract \"About\" section\n",
        "        about_section = soup.find(\"section\", {\"data-section\": \"summary\"})\n",
        "        if about_section:\n",
        "            content_div = about_section.find(\"div\", class_=\"core-section-container__content\")\n",
        "            if content_div:\n",
        "                data.about = content_div.get_text(separator=\"\\n\", strip=True)\n",
        "        else:\n",
        "            # Fallback (legacy structure or alternate layout)\n",
        "            legacy_about = soup.find('section', {'id': 'about'}) or soup.find('div', class_=lambda x: x and 'summary' in x)\n",
        "            if legacy_about:\n",
        "                text_blocks = legacy_about.find_all(text=True)\n",
        "                cleaned_text = ' '.join(t.strip() for t in text_blocks if t.strip())\n",
        "                data.about = cleaned_text or None\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_profile(self, url: str) -> Tuple[Optional[ProfileData], Optional[Dict]]:\n",
        "        try:\n",
        "            html_content = self._fetch_data(url)\n",
        "            if not html_content:\n",
        "                return None, None\n",
        "\n",
        "            profile_data = self.extract_basic_profile_data(html_content)\n",
        "            return profile_data, {\n",
        "                \"name\": profile_data.name,\n",
        "                \"profileUrl\": profile_data.profile_url,\n",
        "                \"profileImage\": profile_data.profile_image,\n",
        "                \"about\": profile_data.about\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting profile data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "# --- Main Processing ---\n",
        "if __name__ == \"__main__\":\n",
        "    input_path = \"pipe1_urls.json\"\n",
        "    output_path = \"pipe1_scrapped.json\"\n",
        "\n",
        "    with open(input_path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    scraper = LinkedInProvider()\n",
        "    final_output = {}\n",
        "\n",
        "    for person_name, profiles in data.items():\n",
        "        print(f\"\\n🔍 Processing: {person_name}\")\n",
        "        enriched_profiles = []\n",
        "\n",
        "        for profile in profiles:\n",
        "            url = profile.get(\"link\")\n",
        "            if not url:\n",
        "                continue\n",
        "\n",
        "            profile_data, raw = scraper.get_profile(url)\n",
        "            if raw:\n",
        "                enriched_profiles.append(raw)\n",
        "\n",
        "        final_output[person_name] = enriched_profiles\n",
        "\n",
        "    with open(output_path, \"w\") as f:\n",
        "        json.dump(final_output, f, indent=2)\n",
        "\n",
        "    print(f\"\\n✅ Done! Output saved to: {output_path}\")\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, List\n",
        "import json\n",
        "import re\n",
        "import difflib\n",
        "from deepface import DeepFace\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load pretrained model\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "class MatchResult(BaseModel):\n",
        "    name: str\n",
        "    linkedin_url: Optional[str] = None\n",
        "    profile_image: Optional[str] = None\n",
        "    confidence_score: float = 0.0\n",
        "    match_type: str = \"none\"\n",
        "\n",
        "\n",
        "def string_similarity(a: str, b: str) -> float:\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
        "\n",
        "\n",
        "def is_static_image(url: str) -> bool:\n",
        "    return \"static.licdn.com\" in url or \"shrinknp\" in url  # Add more patterns if needed\n",
        "\n",
        "\n",
        "def compare_images(input_img_url: str, scraped_img_url: str) -> float:\n",
        "    try:\n",
        "        result = DeepFace.verify(\n",
        "            img1_path=input_img_url,\n",
        "            img2_path=scraped_img_url,\n",
        "            model_name=\"Facenet\",\n",
        "            detector_backend=\"opencv\",\n",
        "            enforce_detection=False\n",
        "        )\n",
        "        return round(1 - result[\"distance\"], 2) if result[\"verified\"] else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"[Image Match Error] {e}\")\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def normalize_image_url(url: str) -> str:\n",
        "    \"\"\"Returns a usable image URL (direct Google Drive or regular link).\"\"\"\n",
        "    if not url:\n",
        "        return \"\"\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        match = re.search(r\"/d/([a-zA-Z0-9_-]+)\", url)\n",
        "        if match:\n",
        "            file_id = match.group(1)\n",
        "            return f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    return url  # assume it's already usable\n",
        "\n",
        "\n",
        "class LinkedInProvider:\n",
        "    def __init__(self, result_dict: Dict):\n",
        "        self.result_dict = result_dict\n",
        "\n",
        "    def get_profile(self, url: str):\n",
        "        for name, entries in self.result_dict.items():\n",
        "            for entry in entries:\n",
        "                if entry.get(\"profileUrl\") == url:\n",
        "                    return entry, {\n",
        "                        \"name\": entry.get(\"name\", \"\"),\n",
        "                        \"profileUrl\": entry.get(\"profileUrl\", \"\"),\n",
        "                        \"profileImage\": entry.get(\"profileImage\", \"\"),\n",
        "                        \"about\": entry.get(\"about\", \"\")\n",
        "                    }\n",
        "        return {}, {}  # Not found\n",
        "\n",
        "\n",
        "def compute_similarity(input_data: Dict, scraped_data: Dict) -> float:\n",
        "    scores = []\n",
        "\n",
        "    if input_data.get(\"name\") and scraped_data.get(\"name\"):\n",
        "        name_score = string_similarity(input_data[\"name\"], scraped_data[\"name\"])\n",
        "        scores.append(name_score)\n",
        "\n",
        "    input_img = normalize_image_url(input_data.get(\"image\", \"\"))\n",
        "    scraped_img = scraped_data.get(\"profileImage\", \"\")\n",
        "    if input_img and scraped_img and not is_static_image(scraped_img):\n",
        "        image_score = compare_images(input_img, scraped_img)\n",
        "        scores.append(image_score)\n",
        "\n",
        "    intro = input_data.get(\"intro\", \"\")\n",
        "    about = scraped_data.get(\"about\", \"\")\n",
        "    if intro and about:\n",
        "        embedding_intro = sbert_model.encode(intro, convert_to_tensor=True)\n",
        "        embedding_about = sbert_model.encode(about, convert_to_tensor=True)\n",
        "        similarity = util.pytorch_cos_sim(embedding_intro, embedding_about).item()\n",
        "        scores.append(similarity)\n",
        "\n",
        "    return round(sum(scores) / len(scores), 2) if scores else 0.0\n",
        "\n",
        "\n",
        "def evaluate_personas(persona_dataset_path: str, linkedin_results_path: str) -> List[Dict]:\n",
        "    with open(persona_dataset_path, \"r\") as f:\n",
        "        personas = json.load(f)\n",
        "\n",
        "    with open(linkedin_results_path, \"r\") as f:\n",
        "        linkedin_results = json.load(f)\n",
        "\n",
        "    provider = LinkedInProvider(result_dict=linkedin_results)\n",
        "    matches = []\n",
        "\n",
        "    for persona in personas:\n",
        "        name = persona[\"name\"]\n",
        "        print(f\"🔍 Processing {name}\")\n",
        "        url_dicts = linkedin_results.get(name, [])\n",
        "        urls = [entry[\"profileUrl\"] for entry in url_dicts if \"profileUrl\" in entry]\n",
        "\n",
        "        best_match = None\n",
        "        best_score = 0.0\n",
        "        best_type = \"none\"\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"🔍 Checking: {url}\")\n",
        "            profile_data, extracted_data = provider.get_profile(url)\n",
        "            if not extracted_data:\n",
        "                print(f\"⛔ No data extracted from {url}\")\n",
        "                continue\n",
        "\n",
        "            score = compute_similarity(input_data=persona, scraped_data=extracted_data)\n",
        "            print(f\"✅ Score: {score} for {url}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                scraped_img = extracted_data.get(\"profileImage\", \"\")\n",
        "                is_static = is_static_image(scraped_img)\n",
        "                has_about = bool(extracted_data.get(\"about\"))\n",
        "\n",
        "                if not is_static and has_about:\n",
        "                    match_type = \"image+text\"\n",
        "                elif not is_static:\n",
        "                    match_type = \"image-only\"\n",
        "                else:\n",
        "                    match_type = \"text-only\" if has_about else \"name-only\"\n",
        "\n",
        "                best_match = MatchResult(\n",
        "                    name=name,\n",
        "                    linkedin_url=extracted_data.get(\"profileUrl\"),\n",
        "                    profile_image=scraped_img,\n",
        "                   confidence_score=score,\n",
        "                    match_type=match_type\n",
        "                )\n",
        "\n",
        "        if best_match:\n",
        "            matches.append(best_match.dict())\n",
        "        else:\n",
        "            matches.append(MatchResult(name=name).dict())\n",
        "\n",
        "    return matches\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = evaluate_personas(\n",
        "        persona_dataset_path=\"pipe_1.json\",\n",
        "        linkedin_results_path=\"pipe1_scrapped.json\"\n",
        "    )\n",
        "\n",
        "    with open(\"pipe1_final.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Processed {len(results)} personas\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i83PF9kR8hV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "with open('pipe_2.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def clean_name(name):\n",
        "    # Remove parenthetical parts and add spacing between camelCase words.\n",
        "    name = re.sub(r\"\\s*\\([^)]*\\)\", \"\", name)\n",
        "    name = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", name)\n",
        "    return name.strip()\n",
        "\n",
        "\n",
        "for person in data:\n",
        "    person['name'] = clean_name(person['name'])\n",
        "\n",
        "\n",
        "with open('cleaned_dataset1.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "#CREATING QUERY\n",
        "import json\n",
        "import re\n",
        "from keybert import KeyBERT\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def generate_query(person):\n",
        "    name = (person.get(\"name\") or \"\").strip()\n",
        "    parts = [name]\n",
        "\n",
        "    return \" \".join(part.strip() for part in parts if part)\n",
        "\n",
        "def process_people(data):\n",
        "    for person in data:\n",
        "        person[\"query\"] = generate_query(person)\n",
        "    return data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"cleaned_dataset1.json\"\n",
        "    output_file = \"processed_dataset1.json\"\n",
        "\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    processed = process_people(data)\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(processed, f, indent=4)\n",
        "\n",
        "    print(\"All entries processed and saved to\", output_file)\n",
        "\n",
        "\"\"\"# Linkedin url search\"\"\"\n",
        "\n",
        "import json\n",
        "import time\n",
        "from typing import Optional, TypedDict\n",
        "import requests\n",
        "\n",
        "class GoogleCustomSearchResponse(TypedDict):\n",
        "    link: str\n",
        "    title: Optional[str]\n",
        "    snippet: Optional[str]\n",
        "\n",
        "class GoogleCustomSearch:\n",
        "    def __init__(self):\n",
        "        self.api_key = \"AIzaSyBhoMDnst3_JoK5TziGClgS27bhX0dQNFk\"\n",
        "        self.search_engine_id = \"c307652d3fbc44ec8\"\n",
        "        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "    def _search(self, query: str, num=10, start=1) -> Optional[dict]:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"key\": self.api_key,\n",
        "            \"cx\": self.search_engine_id,\n",
        "            \"num\": num,\n",
        "            \"start\": start,\n",
        "        }\n",
        "        response = requests.get(self.base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Google Search Failed: {response.status_code} {response.text}\")\n",
        "            return None\n",
        "\n",
        "    def _parse_results(self, response) -> list[GoogleCustomSearchResponse]:\n",
        "        if not response or \"items\" not in response:\n",
        "            return []\n",
        "        results = []\n",
        "        for item in response[\"items\"]:\n",
        "            link = item.get(\"link\", None)\n",
        "            if link:\n",
        "                results.append(\n",
        "                    {\n",
        "                        \"title\": item.get(\"title\", \"\"),\n",
        "                        \"link\": link,\n",
        "                        \"snippet\": item.get(\"snippet\", \"\"),\n",
        "                    }\n",
        "                )\n",
        "        return results\n",
        "\n",
        "    def search(self, query: str, total_results=15) -> list[GoogleCustomSearchResponse]:\n",
        "        all_results = []\n",
        "        remaining = total_results\n",
        "        start = 1\n",
        "        while remaining > 0:\n",
        "            num = min(10, remaining)\n",
        "            response = self._search(query, num=num, start=start)\n",
        "            if not response:\n",
        "                break\n",
        "            parsed = self._parse_results(response)\n",
        "            all_results.extend(parsed)\n",
        "            if len(parsed) < num:\n",
        "                break  # No more results\n",
        "            remaining -= num\n",
        "            start += num\n",
        "            time.sleep(1)  # Be polite to the API\n",
        "        return all_results\n",
        "\n",
        "def main():\n",
        "    # Load personas from processed.json\n",
        "    with open('/content/processed_dataset1.json', 'r', encoding='utf-8') as f:\n",
        "        personas = json.load(f)\n",
        "\n",
        "    searcher = GoogleCustomSearch()\n",
        "    results = {}\n",
        "\n",
        "    for persona in personas:\n",
        "        name = persona.get(\"name\", \"Unknown\")\n",
        "        query = persona.get(\"query\")\n",
        "        print(f\"Searching for: {name} | Query: {query}\")\n",
        "        if not query:\n",
        "            results[name] = []\n",
        "            continue\n",
        "        search_results = searcher.search(f'site:linkedin.com/in {query}', total_results=15)\n",
        "        results[name] = search_results\n",
        "        time.sleep(1)  # Be polite to the API\n",
        "\n",
        "    # Save results to a new JSON file\n",
        "    with open('linkedin_search_results_dataset1.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"Done! Results saved to linkedin_search_results_dataset1.json\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\"\"\"# Linkedin profile details scrapper\"\"\"\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, Tuple\n",
        "import requests\n",
        "import itertools\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# --- Data Models ---\n",
        "class ProfileData(BaseModel):\n",
        "    name: Optional[str] = None\n",
        "    profile_url: Optional[str] = None\n",
        "    profile_image: Optional[str] = None\n",
        "\n",
        "# --- Rotating User Agents ---\n",
        "user_agents = [\n",
        "    \"Slackbot-LinkExpanding 1.0 (+https://api.slack.com/robots)\",\n",
        "    \"LinkedInBot/1.0\",\n",
        "    \"Twitterbot/1.0\",\n",
        "    \"facebookexternalhit/1.1\",\n",
        "    \"WhatsApp/2.0\",\n",
        "    \"Googlebot/2.1 (+http://www.google.com/bot.html)\",\n",
        "]\n",
        "user_agent_cycle = itertools.cycle(user_agents)\n",
        "\n",
        "def mimic_bot_headers() -> str:\n",
        "    \"\"\"Mimic bot headers by cycling through user agents\"\"\"\n",
        "    return next(user_agent_cycle)\n",
        "\n",
        "# --- LinkedIn Scraper ---\n",
        "class LinkedInProvider:\n",
        "    \"\"\"Get basic data (name and image) from LinkedIn URL using web scraping\"\"\"\n",
        "\n",
        "    def _fetch_data(self, url: str) -> Optional[str]:\n",
        "        \"\"\"Fetch HTML content from URL with proxy and rotating user agents\"\"\"\n",
        "        retry_count = 3\n",
        "\n",
        "        for _ in range(retry_count):\n",
        "            user_agent = mimic_bot_headers()\n",
        "\n",
        "            headers = {\n",
        "                \"User-Agent\": user_agent,\n",
        "            }\n",
        "\n",
        "            proxies = {\n",
        "                \"https\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "                \"http\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(\n",
        "                    url,\n",
        "                    headers=headers,\n",
        "                    proxies=proxies,\n",
        "                    timeout=10\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    return response.text\n",
        "            except Exception as e:\n",
        "                print(f\"Request error: {e}\")\n",
        "\n",
        "        print(f\"Failed to fetch the LinkedIn URL after {retry_count} attempts: {url}\")\n",
        "        return None\n",
        "\n",
        "    def extract_basic_profile_data(self, html: str) -> ProfileData:\n",
        "        \"\"\"Extract just name and profile image from HTML\"\"\"\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        data = ProfileData()\n",
        "\n",
        "        # Extract name\n",
        "        name_tag = soup.find('h1', class_=lambda x: x and 'top-card-layout__title' in x)\n",
        "        data.name = name_tag.get_text(strip=True) if name_tag else None\n",
        "\n",
        "        # Extract profile URL\n",
        "        canonical = soup.find('link', rel=\"canonical\")\n",
        "        data.profile_url = canonical['href'] if canonical and canonical.get('href') else None\n",
        "\n",
        "        # Extract profile image\n",
        "        if data.name:\n",
        "            profile_img = soup.find('img', alt=data.name)\n",
        "            if profile_img:\n",
        "                data.profile_image = profile_img.get('data-delayed-url') or profile_img.get('src')\n",
        "\n",
        "        return data\n",
        "\n",
        "    def get_profile(self, url: str) -> Tuple[Optional[ProfileData], Optional[Dict]]:\n",
        "        \"\"\"Get profile data from LinkedIn URL\"\"\"\n",
        "        try:\n",
        "            html_content = self._fetch_data(url)\n",
        "            if not html_content:\n",
        "                return None, None\n",
        "\n",
        "            profile_data = self.extract_basic_profile_data(html_content)\n",
        "\n",
        "            # Return both the structured data and raw extracted data\n",
        "            return profile_data, {\n",
        "                \"name\": profile_data.name,\n",
        "                \"profileUrl\": profile_data.profile_url,\n",
        "                \"profileImage\": profile_data.profile_image\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting profile data: {e}\")\n",
        "            return None, None\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from typing import Optional, Dict, List\n",
        "import json\n",
        "import re\n",
        "import difflib\n",
        "from deepface import DeepFace\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load pretrained model for semantic similarity\n",
        "sbert_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# --- Data Models ---\n",
        "class MatchResult(BaseModel):\n",
        "    name: str\n",
        "    linkedin_url: Optional[str] = None\n",
        "    confidence_score: float = 0.0\n",
        "    match_type: Optional[str] = None\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def string_similarity(a: str, b: str) -> float:\n",
        "    \"\"\"Calculate string similarity using sequence matcher\"\"\"\n",
        "    if not a or not b:\n",
        "        return 0.0\n",
        "    return difflib.SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
        "\n",
        "def compare_images(input_img_url: str, scraped_img_url: str) -> float:\n",
        "    \"\"\"Compare faces in two images using DeepFace\"\"\"\n",
        "    try:\n",
        "        result = DeepFace.verify(\n",
        "            img1_path=input_img_url,\n",
        "            img2_path=scraped_img_url,\n",
        "            model_name=\"Facenet512\",\n",
        "            detector_backend=\"opencv\",\n",
        "            enforce_detection=False\n",
        "        )\n",
        "        return round(1 - result[\"distance\"], 3) if result[\"verified\"] else 0.0\n",
        "    except Exception as e:\n",
        "        print(f\"[Image Match Error] {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def convert_drive_link_to_direct(url: str) -> str:\n",
        "    \"\"\"Convert Google Drive share links to direct download links\"\"\"\n",
        "    if not url:\n",
        "        return \"\"\n",
        "    match = re.search(r\"/d/([a-zA-Z0-9_-]+)\", url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "        return f\"https://drive.google.com/uc?id={file_id}\"\n",
        "    return url\n",
        "\n",
        "# --- Confidence Score Calculation ---\n",
        "def compute_similarity(input_data: Dict, scraped_data: Dict) -> tuple:\n",
        "    \"\"\"Compute similarity score between input data and scraped profile\n",
        "    Returns a tuple of (final_score, match_type, name_score, image_score)\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    name_score = 0.0\n",
        "    image_score = 0.0\n",
        "\n",
        "    has_input_image = \"image\" in input_data and input_data[\"image\"]\n",
        "    has_scraped_image = \"profileImage\" in scraped_data and scraped_data[\"profileImage\"]\n",
        "\n",
        "    # Compare names\n",
        "    if input_data.get(\"name\") and scraped_data.get(\"name\"):\n",
        "        name_score = string_similarity(input_data[\"name\"], scraped_data[\"name\"])\n",
        "        scores[\"name\"] = name_score\n",
        "\n",
        "    # Compare images\n",
        "    if has_input_image and has_scraped_image:\n",
        "        input_img = convert_drive_link_to_direct(input_data[\"image\"])\n",
        "        scraped_img = convert_drive_link_to_direct(scraped_data[\"profileImage\"])\n",
        "\n",
        "        try:\n",
        "            image_score = compare_images(input_img, scraped_img)\n",
        "            scores[\"image\"] = image_score\n",
        "        except Exception as e:\n",
        "            print(f\"[Image Comparison Error] {e}\")\n",
        "\n",
        "    # Determine match type and final score\n",
        "    if \"name\" in scores and \"image\" in scores:\n",
        "        final_score = (scores[\"name\"] * 0.3) + (scores[\"image\"] * 0.7)\n",
        "        match_type = \"name_and_image\"\n",
        "    elif \"name\" in scores:\n",
        "        final_score = min(scores[\"name\"], 0.5)\n",
        "        match_type = \"name_only\"\n",
        "    elif \"image\" in scores:\n",
        "        final_score = min(scores[\"image\"], 0.6)\n",
        "        match_type = \"image_only\"\n",
        "    else:\n",
        "        final_score = 0.0\n",
        "        match_type = \"no_match\"\n",
        "\n",
        "    return round(final_score, 3), match_type, round(name_score, 3), round(image_score, 3)\n",
        "\n",
        "# --- Main Process ---\n",
        "def evaluate_personas(persona_dataset_path: str, linkedin_results_path: str) -> List[Dict]:\n",
        "    \"\"\"Process all personas and find best matches\"\"\"\n",
        "    # Load data\n",
        "    with open(persona_dataset_path, \"r\") as f:\n",
        "        personas = json.load(f)\n",
        "\n",
        "    with open(linkedin_results_path, \"r\") as f:\n",
        "        linkedin_results = json.load(f)\n",
        "\n",
        "    matches = []\n",
        "\n",
        "    for persona in personas:\n",
        "        name = persona[\"name\"]\n",
        "        print(f\"🔍 Processing {name}\")\n",
        "\n",
        "        # Get LinkedIn URLs from search results\n",
        "        url_dicts = linkedin_results.get(name, [])\n",
        "        urls = [entry[\"link\"] for entry in url_dicts]\n",
        "\n",
        "        if not urls:\n",
        "            matches.append(MatchResult(\n",
        "                name=name,\n",
        "                confidence_score=0.0,\n",
        "                name_similarity=0.0,\n",
        "                image_similarity=0.0,\n",
        "                match_type=\"no_results\"\n",
        "            ).dict())\n",
        "            continue\n",
        "\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "        match_type = \"no_match\"\n",
        "\n",
        "        provider = LinkedInProvider()\n",
        "\n",
        "        first_url = urls[0]\n",
        "        first_profile_data = None\n",
        "        first_extracted_data = None\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"🔍 Scraping: {url}\")\n",
        "            try:\n",
        "                profile_data, extracted_data = provider.get_profile(url)\n",
        "\n",
        "                if url == first_url:\n",
        "                    first_profile_data = profile_data\n",
        "                    first_extracted_data = extracted_data\n",
        "\n",
        "                if not profile_data or not extracted_data:\n",
        "                    continue\n",
        "\n",
        "                score, current_match_type, name_sim, image_sim = compute_similarity(\n",
        "                    input_data=persona,\n",
        "                    scraped_data=extracted_data\n",
        "                )\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    match_type = current_match_type\n",
        "                    best_name_score = name_sim\n",
        "                    best_image_score = image_sim\n",
        "\n",
        "                    best_match = MatchResult(\n",
        "                        name=name,\n",
        "                        linkedin_url=extracted_data.get(\"profileUrl\", url),\n",
        "                        confidence_score=score,\n",
        "                        match_type=match_type\n",
        "                    )\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[Scraping Error] {e}\")\n",
        "                continue\n",
        "\n",
        "        has_input_image = \"image\" in persona and persona[\"image\"]\n",
        "\n",
        "        if not best_match:\n",
        "            if first_extracted_data:\n",
        "                name_score = string_similarity(name, first_extracted_data.get(\"name\", \"\"))\n",
        "                if not has_input_image:\n",
        "                    adjusted_score = min(name_score, 0.4)\n",
        "                    fallback_type = \"name_only_fallback\"\n",
        "                else:\n",
        "                    adjusted_score = min(name_score * 0.4, 0.4)\n",
        "                    fallback_type = \"partial_match_fallback\"\n",
        "\n",
        "                best_match = MatchResult(\n",
        "                    name=name,\n",
        "                    linkedin_url=first_extracted_data.get(\"profileUrl\", first_url),\n",
        "                    confidence_score=round(adjusted_score, 2),\n",
        "                    match_type=fallback_type\n",
        "                )\n",
        "            else:\n",
        "                best_match = MatchResult(\n",
        "                    name=name,\n",
        "                    linkedin_url=first_url,\n",
        "                    confidence_score=0.1,\n",
        "                    match_type=\"url_only_fallback\"\n",
        "                )\n",
        "\n",
        "        matches.append(best_match.dict())\n",
        "\n",
        "    return matches\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = evaluate_personas(\n",
        "        persona_dataset_path=\"pipe_2.json\",\n",
        "        linkedin_results_path=\"linkedin_search_results_dataset1.json\"\n",
        "    )\n",
        "\n",
        "    # Save results\n",
        "    with open(\"pipe2_final.json\", \"w\") as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(f\"✅ Processed {len(results)} personas and saved results.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CUMDdmD98hRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"pipe_3.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Helper function to check if a value is empty\n",
        "def is_empty(val):\n",
        "    return (\n",
        "        val is None or\n",
        "        (isinstance(val, str) and val.strip() == \"\") or\n",
        "        (isinstance(val, list) and len(val) == 0)\n",
        "    )\n",
        "\n",
        "# Clean & flexible filtering logic\n",
        "filtered_profiles = [\n",
        "    profile for profile in data\n",
        "    if is_empty(profile.get(\"intro\")) and (\n",
        "        not is_empty(profile.get(\"company_industry\")) or\n",
        "        not is_empty(profile.get(\"company_size\"))\n",
        "    )\n",
        "]\n",
        "\n",
        "# Save to pipe3.json\n",
        "with open(\"pipe3_processed.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(filtered_profiles, outfile, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved {len(filtered_profiles)} profiles to pipe3.json\")\n",
        "\n",
        "import json\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"pipe3_processed.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Update the 'query' field\n",
        "for profile in data:\n",
        "    name = profile.get(\"name\", \"\")\n",
        "    industry = profile.get(\"company_industry\")\n",
        "\n",
        "    # Check if industry is valid and non-empty\n",
        "    if industry and isinstance(industry, str) and industry.strip():\n",
        "        updated_query = f\"{name} {industry.strip().lower()}\"\n",
        "    else:\n",
        "        updated_query = name  # If no industry, keep only the name\n",
        "\n",
        "    profile[\"query\"] = updated_query\n",
        "\n",
        "# Save the updated dataset (optional: you can overwrite or save as new)\n",
        "with open(\"pipe3_query.json\", \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(data, outfile, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(\"Updated 'query' field based on name and company_industry.\")\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "def extract_range(size_str):\n",
        "    if not size_str:\n",
        "        return None\n",
        "\n",
        "    size_str = size_str.lower()\n",
        "    numbers = list(map(int, re.findall(r'\\d+', size_str)))\n",
        "\n",
        "    if len(numbers) == 2:\n",
        "        return f\"{numbers[0]}-{numbers[1]}\"\n",
        "    elif len(numbers) == 1:\n",
        "        if any(kw in size_str for kw in ['fewer', 'less', 'under']):\n",
        "            return f\"0-{numbers[0]}\"\n",
        "        else:\n",
        "            return f\"{numbers[0]}-{numbers[0]}\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Load your JSON (replace with actual file or string load)\n",
        "with open('pipe3_query.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Update company_size\n",
        "for entry in data:\n",
        "    entry['company_size'] = extract_range(entry.get('company_size'))\n",
        "\n",
        "# Save the updated JSON\n",
        "with open('processed_3.json', 'w') as f:\n",
        "    json.dump(data, f, indent=4)\n",
        "\n",
        "print(\"Updated JSON saved as 'processed.json'\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from typing import Optional, TypedDict, List\n",
        "\n",
        "class GoogleCustomSearchResponse(TypedDict):\n",
        "    link: str\n",
        "    title: Optional[str]\n",
        "    snippet: Optional[str]\n",
        "\n",
        "class GoogleCustomSearch:\n",
        "    def __init__(self):  # Fixed: should be __init__\n",
        "        self.api_key = \"AIzaSyBhoMDnst3_JoK5TziGClgS27bhX0dQNFk\"\n",
        "        self.search_engine_id = \"c307652d3fbc44ec8\"\n",
        "        self.base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "    def _search(self, query: str, num=10, start=1) -> Optional[dict]:\n",
        "        params = {\n",
        "            \"q\": query,\n",
        "            \"key\": self.api_key,\n",
        "            \"cx\": self.search_engine_id,\n",
        "            \"num\": num,\n",
        "            \"start\": start,\n",
        "        }\n",
        "        response = requests.get(self.base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Google Search Failed: {response.status_code} {response.text}\")\n",
        "            return None\n",
        "\n",
        "    def _parse_results(self, response) -> List[GoogleCustomSearchResponse]:\n",
        "        if not response or \"items\" not in response:\n",
        "            return []\n",
        "        results = []\n",
        "        for item in response[\"items\"]:\n",
        "            link = item.get(\"link\", None)\n",
        "            if link:\n",
        "                results.append({\n",
        "                    \"title\": item.get(\"title\", \"\"),\n",
        "                    \"link\": link,\n",
        "                    \"snippet\": item.get(\"snippet\", \"\"),\n",
        "                })\n",
        "        return results\n",
        "\n",
        "    def search(self, query: str, num=5, start=1) -> List[GoogleCustomSearchResponse]:\n",
        "        response = self._search(query, num, start)\n",
        "        if response:\n",
        "            return self._parse_results(response)\n",
        "        return []\n",
        "\n",
        "def main():\n",
        "    # Load personas from processed.json\n",
        "    with open('processed_3.json', 'r', encoding='utf-8') as f:\n",
        "        personas = json.load(f)\n",
        "\n",
        "    searcher = GoogleCustomSearch()\n",
        "    results = {}\n",
        "\n",
        "    for persona in personas:\n",
        "        name = persona.get(\"name\", \"Unknown\")\n",
        "        query = persona.get(\"query\")\n",
        "        print(f\"Searching for: {name} | Query: {query}\")\n",
        "        if not query:\n",
        "            results[name] = []\n",
        "            continue\n",
        "        search_results = searcher.search(f'site:linkedin.com/in {query}', num=10)\n",
        "        results[name] = search_results\n",
        "        time.sleep(1)  # Be polite to the API\n",
        "\n",
        "    # Save results to a new JSON file\n",
        "    with open('pipe3_urls.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    print(\"Done! Results saved to pipe3_urls.json\")\n",
        "\n",
        "if __name__ == \"__main__\":  # Fixed: should be __name__\n",
        "    main()\n",
        "\n",
        "import json\n",
        "import re\n",
        "import itertools\n",
        "from typing_extensions import Optional, TypedDict\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# List of user agents used in rotation\n",
        "user_agents = [\n",
        "    \"Slackbot-LinkExpanding 1.0 (+https://api.slack.com/robots)\",\n",
        "    \"LinkedInBot/1.0\",\n",
        "    \"Twitterbot/1.0\",\n",
        "    \"facebookexternalhit/1.1\",\n",
        "    \"WhatsApp/2.0\",\n",
        "    \"Googlebot/2.1 (+http://www.google.com/bot.html)\",\n",
        "]\n",
        "\n",
        "user_agent_cycle = itertools.cycle(user_agents)\n",
        "\n",
        "\n",
        "def mimic_bot_headers() -> str:\n",
        "    \"\"\"\n",
        "    Mimic bot headers\n",
        "    \"\"\"\n",
        "    return next(user_agent_cycle)\n",
        "\n",
        "\n",
        "def get_first_last_name(name: str) -> tuple[str, Optional[str]]:\n",
        "    \"\"\"\n",
        "    Extracts first and last name from a full name.\n",
        "    \"\"\"\n",
        "    name_parts = name.split(\" \")\n",
        "    first_name = name_parts[0]\n",
        "    last_name = \" \".join(name_parts[1:]) if len(name_parts) > 1 else None\n",
        "    return first_name, last_name\n",
        "\n",
        "\n",
        "# TypedDict and Pydantic models for profile data\n",
        "class Workspace(TypedDict):\n",
        "    name: str\n",
        "    url: Optional[str]\n",
        "\n",
        "\n",
        "class LinkedinPersonProfile(BaseModel):\n",
        "    first_name: Optional[str]\n",
        "    last_name: Optional[str]\n",
        "    linkedin: Optional[str]\n",
        "    workspaces: Optional[list[Workspace]]\n",
        "\n",
        "\n",
        "class LinkedinCompanyProfile(BaseModel):\n",
        "    name: Optional[str]\n",
        "    website: Optional[str]\n",
        "    description: Optional[str]\n",
        "    address: Optional[str]\n",
        "    number_of_employees: Optional[int]\n",
        "\n",
        "\n",
        "class LinkedInProvider:\n",
        "    \"\"\"\n",
        "    Get data from a LinkedIn URL using web scraping.\n",
        "    \"\"\"\n",
        "\n",
        "    def _fetch_data(self, url: str) -> Optional[str]:\n",
        "        retry_count = 3\n",
        "        for _ in range(retry_count):\n",
        "            user_agent = mimic_bot_headers()\n",
        "            headers = {\"User-Agent\": user_agent}\n",
        "            proxies = {\n",
        "                \"https\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "                \"http\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, proxies=proxies)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "        print(f\"Failed to fetch the LinkedIn URL: {url}\")\n",
        "        return None\n",
        "\n",
        "    def _json_ld_data(self, html_content: str) -> Optional[dict]:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            script_tag = soup.find(\"script\", {\"type\": \"application/ld+json\"})\n",
        "            json_ld_data = json.loads(script_tag.string) if script_tag else {}\n",
        "            return json_ld_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting JSON-LD data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def person_profile(self, url: str) -> Optional[tuple[LinkedinPersonProfile, dict]]:\n",
        "        \"\"\"\n",
        "        Extracts the profile details of a person.\n",
        "        Returns a tuple containing the Pydantic model and additional scraped data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            html_content = self._fetch_data(url)\n",
        "            if not html_content:\n",
        "                return None\n",
        "\n",
        "            # Use a helper function to scrape additional profile details\n",
        "            data_obj = extract_profile_data(html_content)\n",
        "            json_ld_data = self._json_ld_data(html_content)\n",
        "\n",
        "            if json_ld_data:\n",
        "                # If JSON-LD data represents a ProfilePage, use \"mainEntity\", otherwise iterate the graph\n",
        "                if json_ld_data.get(\"@type\") == \"ProfilePage\":\n",
        "                    person_data = json_ld_data[\"mainEntity\"]\n",
        "                else:\n",
        "                    person_data = next(\n",
        "                        (\n",
        "                            item\n",
        "                            for item in json_ld_data.get(\"@graph\", [])\n",
        "                            if item.get(\"@type\") == \"Person\"\n",
        "                        ),\n",
        "                        {},\n",
        "                    )\n",
        "\n",
        "                name = person_data.get(\"name\")\n",
        "                workplaces = [\n",
        "                    {\"name\": org.get(\"name\"), \"url\": org.get(\"url\")}\n",
        "                    for org in person_data.get(\"worksFor\", [])\n",
        "                    if \"name\" in org\n",
        "                ]\n",
        "                first_name, last_name = (get_first_last_name(name) if name else (None, None))\n",
        "\n",
        "                profile = LinkedinPersonProfile(\n",
        "                    first_name=first_name,\n",
        "                    last_name=last_name,\n",
        "                    linkedin=url,\n",
        "                    workspaces=workplaces,\n",
        "                )\n",
        "                return profile, data_obj\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting person profile: {e}\")\n",
        "            return None\n",
        "\n",
        "    def company_profile(self, username: str) -> Optional[LinkedinCompanyProfile]:\n",
        "        \"\"\"\n",
        "        Extracts the profile details of a company.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = f\"https://www.linkedin.com/company/{username}\"\n",
        "            html_content = self._fetch_data(url)\n",
        "            if not html_content:\n",
        "                return None\n",
        "\n",
        "            json_ld_data = self._json_ld_data(html_content)\n",
        "            if json_ld_data:\n",
        "                if json_ld_data.get(\"@type\") == \"ProfilePage\":\n",
        "                    organization_data = json_ld_data[\"mainEntity\"]\n",
        "                else:\n",
        "                    organization_data = next(\n",
        "                        (\n",
        "                            item\n",
        "                            for item in json_ld_data.get(\"@graph\", [])\n",
        "                            if item.get(\"@type\") == \"Organization\"\n",
        "                        ),\n",
        "                        {},\n",
        "                    )\n",
        "\n",
        "                name = organization_data.get(\"name\")\n",
        "                website = organization_data.get(\"sameAs\")\n",
        "                description = organization_data.get(\"description\")\n",
        "                number_of_employees = organization_data.get(\"numberOfEmployees\", {}).get(\"value\")\n",
        "\n",
        "                # Format the address\n",
        "                address_dict = organization_data.get(\"address\", {})\n",
        "                address_parts = [\n",
        "                    address_dict.get(\"streetAddress\"),\n",
        "                    address_dict.get(\"addressLocality\"),\n",
        "                    address_dict.get(\"addressRegion\"),\n",
        "                    address_dict.get(\"postalCode\"),\n",
        "                    address_dict.get(\"addressCountry\"),\n",
        "                ]\n",
        "                address = \", \".join(filter(None, address_parts))\n",
        "                return LinkedinCompanyProfile(\n",
        "                    name=name,\n",
        "                    website=website,\n",
        "                    description=description,\n",
        "                    address=address,\n",
        "                    number_of_employees=number_of_employees,\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting company profile: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def extract_profile_data(html: str) -> dict:\n",
        "    \"\"\"\n",
        "    Scrapes additional profile data from HTML.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    data = {}\n",
        "\n",
        "    name_tag = soup.find(\"h1\", class_=lambda x: x and \"top-card-layout__title\" in x)\n",
        "    data[\"name\"] = name_tag.get_text(strip=True) if name_tag else None\n",
        "\n",
        "    canonical = soup.find(\"link\", rel=\"canonical\")\n",
        "    data[\"profileUrl\"] = canonical[\"href\"] if canonical and canonical.get(\"href\") else None\n",
        "\n",
        "    profile_img = soup.find(\"img\", alt=data[\"name\"])\n",
        "    if profile_img:\n",
        "        data[\"profileImage\"] = profile_img.get(\"data-delayed-url\") or profile_img.get(\"src\")\n",
        "    else:\n",
        "        data[\"profileImage\"] = None\n",
        "\n",
        "    about_section = soup.find(\"section\", class_=lambda x: x and \"summary\" in x)\n",
        "    if about_section:\n",
        "        p_about = about_section.find(\"p\")\n",
        "        data[\"about\"] = p_about.get_text(strip=True) if p_about else None\n",
        "    else:\n",
        "        data[\"about\"] = None\n",
        "\n",
        "    # Extract experience items\n",
        "    exp_items = []\n",
        "    exp_section = soup.find(\"section\", attrs={\"data-section\": \"experience\"})\n",
        "    if exp_section:\n",
        "        for li in exp_section.find_all(\"li\", class_=lambda x: x and \"experience-item\" in x):\n",
        "            company = None\n",
        "            location = None\n",
        "            company_tag = li.find(\"span\", class_=lambda x: x and \"experience-item__subtitle\" in x)\n",
        "            if company_tag:\n",
        "                company = company_tag.get_text(strip=True)\n",
        "            location_tag = li.find(\"p\", class_=lambda x: x and \"experience-item__meta-item\" in x)\n",
        "            if location_tag:\n",
        "                location = location_tag.get_text(strip=True)\n",
        "            if company or location:\n",
        "                exp_items.append({\"company\": company, \"location\": location})\n",
        "    data[\"experience\"] = exp_items\n",
        "\n",
        "    # Extract education items\n",
        "    edu_items = []\n",
        "    edu_section = soup.find(\"section\", attrs={\"data-section\": \"educationsDetails\"})\n",
        "    if edu_section:\n",
        "        for li in edu_section.find_all(\"li\", class_=lambda x: x and \"education__list-item\" in x):\n",
        "            institution = None\n",
        "            period = None\n",
        "            description = None\n",
        "            inst_link = li.find(\"a\", href=lambda href: href and \"school\" in href)\n",
        "            if inst_link:\n",
        "                institution = inst_link.get_text(strip=True)\n",
        "            period_tag = li.find(\"span\", class_=lambda x: x and \"date-range\" in x)\n",
        "            if period_tag:\n",
        "                period = period_tag.get_text(strip=True)\n",
        "            desc_div = li.find(\"div\", class_=lambda x: x and \"show-more-less-text\" in x)\n",
        "            if desc_div:\n",
        "                description = desc_div.get_text(\" \", strip=True)\n",
        "            edu_items.append({\"institution\": institution, \"period\": period, \"description\": description})\n",
        "    data[\"education\"] = edu_items\n",
        "\n",
        "    # Extract languages\n",
        "    languages = []\n",
        "    lang_section = soup.find(\"section\", class_=lambda x: x and \"languages\" in x)\n",
        "    if lang_section:\n",
        "        for li in lang_section.find_all(\"li\"):\n",
        "            language = None\n",
        "            proficiency = None\n",
        "            lang_name_tag = li.find(\"h3\")\n",
        "            prof_tag = li.find(\"h4\")\n",
        "            if lang_name_tag:\n",
        "                language = lang_name_tag.get_text(strip=True)\n",
        "            if prof_tag:\n",
        "                proficiency = prof_tag.get_text(strip=True)\n",
        "            if language:\n",
        "                languages.append({\"language\": language, \"proficiency\": proficiency})\n",
        "    data[\"languages\"] = languages\n",
        "\n",
        "    # Extract recommendations count if available\n",
        "    recommendations_received = None\n",
        "    rec_section = soup.find(\"section\", class_=lambda x: x and \"recommendations\" in x)\n",
        "    if rec_section:\n",
        "        rec_text = rec_section.get_text(\" \", strip=True)\n",
        "        m = re.search(r\"(\\d+)\\s+people\\s+have\\s+recommended\", rec_text)\n",
        "        if m:\n",
        "            recommendations_received = int(m.group(1))\n",
        "    data[\"recommendationsReceived\"] = recommendations_received\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_urls_from_file(input_file: str, output_file: str):\n",
        "    \"\"\"\n",
        "    Reads a JSON file with LinkedIn URLs, processes each URL to extract profile data,\n",
        "    and writes the results to an output JSON file.\n",
        "    \"\"\"\n",
        "    # Load the JSON file containing URLs.\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        url_data = json.load(f)\n",
        "\n",
        "    profiles = {}\n",
        "    provider = LinkedInProvider()\n",
        "\n",
        "    for person_name, entries in url_data.items():\n",
        "        profiles[person_name] = []\n",
        "        for entry in entries:\n",
        "            link = entry.get(\"link\")\n",
        "            if link:\n",
        "                print(f\"Processing URL: {link}\")\n",
        "                profile_result = provider.person_profile(link)\n",
        "                if profile_result:\n",
        "                    profile_obj, complete_data = profile_result\n",
        "                    profiles[person_name].append({\n",
        "                        \"profile\": profile_obj.dict(),\n",
        "                        \"scraped_data\": complete_data,\n",
        "                        \"source_title\": entry.get(\"title\"),\n",
        "                        \"source_snippet\": entry.get(\"snippet\")\n",
        "                    })\n",
        "                else:\n",
        "                    profiles[person_name].append({\n",
        "                        \"error\": f\"Could not retrieve data for {link}\",\n",
        "                        \"source_title\": entry.get(\"title\")\n",
        "                    })\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(profiles, f, indent=2)\n",
        "\n",
        "    print(f\"Processed profiles have been saved to {output_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_json_file = \"pipe3_urls.json\"\n",
        "    output_json_file = \"pipe3_scrapped.json\"\n",
        "    process_urls_from_file(input_json_file, output_json_file)\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"pipe3_scrapped.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "output = {}\n",
        "\n",
        "for main_name, profiles in data.items():\n",
        "    new_profiles = []\n",
        "    for profile_entry in profiles:\n",
        "        profile = profile_entry.get(\"profile\", {})\n",
        "        scraped_data = profile_entry.get(\"scraped_data\", {})\n",
        "\n",
        "        name = f\"{profile.get('first_name', '')} {profile.get('last_name', '')}\".strip()\n",
        "        linkedin = profile.get(\"linkedin\")\n",
        "        profile_image = scraped_data.get(\"profileImage\")\n",
        "\n",
        "        workspaces = profile.get(\"workspaces\", [])\n",
        "        companies = [\n",
        "            {\"name\": ws[\"name\"], \"url\": ws[\"url\"]}\n",
        "            for ws in workspaces if ws.get(\"url\")\n",
        "        ]\n",
        "\n",
        "        new_profiles.append({\n",
        "            \"name\": name,\n",
        "            \"linkedin\": linkedin,\n",
        "            \"profile_image\": profile_image,\n",
        "            \"companies\": companies\n",
        "        })\n",
        "\n",
        "    output[main_name] = new_profiles\n",
        "\n",
        "with open(\"pipe3_companies.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "\n",
        "print(\"Done! Saved to pipe3_companies.json\")\n",
        "\n",
        "import json\n",
        "import re\n",
        "import itertools\n",
        "from typing_extensions import Optional\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of user agents used in rotation\n",
        "user_agents = [\n",
        "    \"Slackbot-LinkExpanding 1.0 (+https://api.slack.com/robots)\",\n",
        "    \"LinkedInBot/1.0\",\n",
        "    \"Twitterbot/1.0\",\n",
        "    \"facebookexternalhit/1.1\",\n",
        "    \"WhatsApp/2.0\",\n",
        "    \"Googlebot/2.1 (+http://www.google.com/bot.html)\",\n",
        "]\n",
        "\n",
        "user_agent_cycle = itertools.cycle(user_agents)\n",
        "\n",
        "\n",
        "def mimic_bot_headers() -> str:\n",
        "    \"\"\"Mimic bot headers\"\"\"\n",
        "    return next(user_agent_cycle)\n",
        "\n",
        "\n",
        "class LinkedinCompanyProfile(BaseModel):\n",
        "    name: Optional[str]\n",
        "    website: Optional[str]\n",
        "    description: Optional[str]\n",
        "    address: Optional[str]\n",
        "    number_of_employees: Optional[int]\n",
        "\n",
        "\n",
        "class LinkedInProvider:\n",
        "    \"\"\"\n",
        "    Get data from a LinkedIn company URL using web scraping.\n",
        "    \"\"\"\n",
        "\n",
        "    def _fetch_data(self, url: str) -> Optional[str]:\n",
        "        retry_count = 3\n",
        "        for _ in range(retry_count):\n",
        "            user_agent = mimic_bot_headers()\n",
        "            headers = {\"User-Agent\": user_agent}\n",
        "            proxies = {\n",
        "                \"https\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "                \"http\": \"http://brd-customer-hl_6c1f36a6-zone-datacenter_proxy2:1qyqs0lnh5zi@brd.superproxy.io:33335\",\n",
        "            }\n",
        "            response = requests.get(url, headers=headers, proxies=proxies)\n",
        "            if response.status_code == 200:\n",
        "                return response.text\n",
        "        print(f\"Failed to fetch the LinkedIn URL: {url}\")\n",
        "        return None\n",
        "\n",
        "    def _json_ld_data(self, html_content: str) -> Optional[dict]:\n",
        "        try:\n",
        "            soup = BeautifulSoup(html_content, \"html.parser\")\n",
        "            script_tag = soup.find(\"script\", {\"type\": \"application/ld+json\"})\n",
        "            json_ld_data = json.loads(script_tag.string) if script_tag else {}\n",
        "            return json_ld_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting JSON-LD data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def company_profile(self, username: str) -> Optional[LinkedinCompanyProfile]:\n",
        "        \"\"\"\n",
        "        Extracts the profile details of a company.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url = f\"https://www.linkedin.com/company/{username}\"\n",
        "            html_content = self._fetch_data(url)\n",
        "            if not html_content:\n",
        "                return None\n",
        "\n",
        "            json_ld_data = self._json_ld_data(html_content)\n",
        "            if json_ld_data:\n",
        "                if json_ld_data.get(\"@type\") == \"ProfilePage\":\n",
        "                    organization_data = json_ld_data[\"mainEntity\"]\n",
        "                else:\n",
        "                    organization_data = next(\n",
        "                        (\n",
        "                            item\n",
        "                            for item in json_ld_data.get(\"@graph\", [])\n",
        "                            if item.get(\"@type\") == \"Organization\"\n",
        "                        ),\n",
        "                        {},\n",
        "                    )\n",
        "\n",
        "                name = organization_data.get(\"name\")\n",
        "                website = organization_data.get(\"sameAs\")\n",
        "                description = organization_data.get(\"description\")\n",
        "                number_of_employees = organization_data.get(\"numberOfEmployees\", {}).get(\"value\")\n",
        "\n",
        "                address_dict = organization_data.get(\"address\", {})\n",
        "                address_parts = [\n",
        "                    address_dict.get(\"streetAddress\"),\n",
        "                    address_dict.get(\"addressLocality\"),\n",
        "                    address_dict.get(\"addressRegion\"),\n",
        "                    address_dict.get(\"postalCode\"),\n",
        "                    address_dict.get(\"addressCountry\"),\n",
        "                ]\n",
        "                address = \", \".join(filter(None, address_parts))\n",
        "\n",
        "                return LinkedinCompanyProfile(\n",
        "                    name=name,\n",
        "                    website=website,\n",
        "                    description=description,\n",
        "                    address=address,\n",
        "                    number_of_employees=number_of_employees,\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"Error in extracting company profile: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def process_company_urls_from_file(input_file: str, output_file: str):\n",
        "    \"\"\"\n",
        "    Reads a JSON file with company URLs, processes each URL to extract company profile data,\n",
        "    and writes the results to an output JSON file.\n",
        "    Includes the person's name and LinkedIn profile URL in each record.\n",
        "    \"\"\"\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        input_data = json.load(f)\n",
        "\n",
        "    provider = LinkedInProvider()\n",
        "    output_data = {}\n",
        "\n",
        "    for person_name, entries in input_data.items():\n",
        "        output_data[person_name] = []\n",
        "\n",
        "        for entry in entries:\n",
        "            user_name = entry.get(\"name\", \"\")\n",
        "            user_profile_url = entry.get(\"url\", \"\")\n",
        "            companies = entry.get(\"companies\", [])\n",
        "\n",
        "            for company in companies:\n",
        "                company_url = company.get(\"url\")\n",
        "                if company_url and \"linkedin.com/company/\" in company_url:\n",
        "                    match = re.search(r\"linkedin\\.com/company/([^/?#]+)\", company_url)\n",
        "                    if match:\n",
        "                        company_username = match.group(1)\n",
        "                        print(f\"Processing company: {company_username}\")\n",
        "                        profile = provider.company_profile(company_username)\n",
        "\n",
        "                        base_data = {\n",
        "                            \"name\": user_name,\n",
        "                            \"linkedin_profile_url\": user_profile_url,\n",
        "                            \"company_url\": company_url\n",
        "                        }\n",
        "\n",
        "                        if profile:\n",
        "                          output_data[person_name].append({\n",
        "    \"company_name\": profile.name,\n",
        "    \"company_url\": company_url,\n",
        "    \"person_name\": person_name,\n",
        "    \"person_profile_url\": entry.get(\"linkedin\", \"Unknown\"),\n",
        "    \"person_image_url\": entry.get(\"profile_image\", \"Unknown\"),\n",
        "    \"company_profile\": profile.dict()\n",
        "})\n",
        "\n",
        "\n",
        "                        else:\n",
        "                            output_data[person_name].append({\n",
        "                                **base_data,\n",
        "                                \"error\": f\"Failed to extract company profile for {company_username}\"\n",
        "                            })\n",
        "\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "\n",
        "    print(f\"Company profiles saved to {output_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_json_file = \"pipe3_companies.json\"  # Your input file\n",
        "    output_json_file = \"pipe3_company_profiles_output.json\"\n",
        "    process_company_urls_from_file(input_json_file, output_json_file)\n",
        "\n",
        "import json\n",
        "\n",
        "# Helper to convert a string like \"0-50\" into a numeric range\n",
        "def parse_range(size_str):\n",
        "    if not size_str or \"-\" not in size_str:\n",
        "        return None, None\n",
        "    try:\n",
        "        low, high = size_str.split(\"-\")\n",
        "        return int(low), int(high)\n",
        "    except ValueError:\n",
        "        return None, None\n",
        "\n",
        "# Load profile and company data\n",
        "with open(\"processed_3.json\", \"r\") as f:\n",
        "    profiles = json.load(f)\n",
        "\n",
        "with open(\"pipe3_company_profiles_output.json\", \"r\") as f:\n",
        "    matches = json.load(f)\n",
        "\n",
        "# Validate matches\n",
        "results = []\n",
        "\n",
        "for profile in profiles:\n",
        "    name = profile.get(\"name\")\n",
        "    size_range = profile.get(\"company_size\")\n",
        "\n",
        "    low, high = parse_range(size_range)\n",
        "\n",
        "    if name not in matches:\n",
        "        continue  # skip if no companies found\n",
        "\n",
        "    person_matches = matches[name]\n",
        "    for match in person_matches:\n",
        "        company = match.get(\"company_name\")\n",
        "        company_profile = match.get(\"company_profile\", {})\n",
        "        num_employees = company_profile.get(\"number_of_employees\")\n",
        "        person_url = match.get(\"person_profile_url\")\n",
        "\n",
        "        # Check if the range is missing\n",
        "        if low is None or high is None:\n",
        "            results.append({\n",
        "                \"person_name\": name,\n",
        "                \"person_profile_url\": person_url,\n",
        "                \"company_name\": company,\n",
        "                \"num_employees\": num_employees,\n",
        "                \"expected_range\": size_range,\n",
        "                \"match\": False\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        # Compare number of employees against expected range\n",
        "        if isinstance(num_employees, int):\n",
        "            in_range = low <= num_employees <= high\n",
        "            results.append({\n",
        "                \"person_name\": name,\n",
        "                \"person_profile_url\": person_url,\n",
        "                \"company_name\": company,\n",
        "                \"num_employees\": num_employees,\n",
        "                \"expected_range\": size_range,\n",
        "                \"match\": in_range\n",
        "            })\n",
        "\n",
        "# Output results\n",
        "with open(\"pipe3_company_size_validation_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Validation complete. Results saved to pipe3_company_size_validation_results.json.\")\n",
        "\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load data\n",
        "with open(\"pipe3_company_size_validation_results.json\", \"r\") as f:\n",
        "    validation_data = json.load(f)\n",
        "\n",
        "with open(\"pipe3_urls.json\", \"r\") as f:\n",
        "    linkedin_search = json.load(f)\n",
        "\n",
        "# Helper to parse range like \"0-50\"\n",
        "def parse_range(size_str):\n",
        "    try:\n",
        "        low, high = map(int, size_str.split(\"-\"))\n",
        "        return low, high\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "# Group entries by person\n",
        "grouped = defaultdict(list)\n",
        "for entry in validation_data:\n",
        "    grouped[entry[\"person_name\"]].append(entry)\n",
        "\n",
        "final_output = []\n",
        "\n",
        "for name, records in grouped.items():\n",
        "    # Step 1: Filter records where match is True\n",
        "    true_matches = [r for r in records if r[\"match\"]]\n",
        "\n",
        "    if true_matches:\n",
        "        # Step 2: Pick best match based on closest to expected range average\n",
        "        low, high = parse_range(true_matches[0][\"expected_range\"])\n",
        "        mid = (low + high) / 2 if low is not None and high is not None else 0\n",
        "\n",
        "        best_match = min(true_matches, key=lambda r: abs(r[\"num_employees\"] - mid))\n",
        "\n",
        "        # Step 3: Calculate similarity using improved formula\n",
        "        if mid > 0:\n",
        "            deviation = abs((mid - best_match[\"num_employees\"]) / mid) * 100\n",
        "            similarity = max(0, 100 - deviation)\n",
        "        else:\n",
        "            similarity = 0\n",
        "\n",
        "        final_output.append({\n",
        "            \"person_name\": name,\n",
        "            \"match\": True,\n",
        "            \"person_profile_url\": best_match[\"person_profile_url\"],\n",
        "            \"expected_range\": best_match[\"expected_range\"],\n",
        "            \"similarity\": round(similarity, 2)\n",
        "        })\n",
        "    else:\n",
        "        # Step 4: No match — determine similarity if range exists, else fallback\n",
        "        expected_range = records[0][\"expected_range\"]\n",
        "\n",
        "        if expected_range:\n",
        "            low, high = parse_range(expected_range)\n",
        "            mid = (low + high) / 2 if low is not None and high is not None else 0\n",
        "\n",
        "            best_guess = min(\n",
        "                records,\n",
        "                key=lambda r: abs(r[\"num_employees\"] - mid) if mid else float('inf')\n",
        "            )\n",
        "\n",
        "            if mid > 0:\n",
        "                deviation = abs((mid - best_guess[\"num_employees\"]) / mid) * 100\n",
        "                similarity = max(0, 100 - deviation)\n",
        "            else:\n",
        "                similarity = 0\n",
        "\n",
        "            profile_url = best_guess[\"person_profile_url\"]\n",
        "        else:\n",
        "            similarity = 0\n",
        "            profile_url = linkedin_search.get(name, [{}])[0].get(\"link\", None)\n",
        "\n",
        "        final_output.append({\n",
        "            \"person_name\": name,\n",
        "            \"match\": False,\n",
        "            \"person_profile_url\": profile_url,\n",
        "            \"expected_range\": expected_range,\n",
        "            \"similarity\": round(similarity, 2)\n",
        "        })\n",
        "\n",
        "# Save result\n",
        "with open(\"pipe3_final_companysize_validation_output.json\", \"w\") as f:\n",
        "    json.dump(final_output, f, indent=2)\n",
        "\n",
        "print(\"Final output saved to pipe3_final_companysize_validation_output.json\")\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load JSONs\n",
        "with open(\"pipe3_company_profiles_output.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    linkedin_data = json.load(f)\n",
        "\n",
        "with open(\"processed_3.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "# Build train_images and train_profile_urls from linkedin_data\n",
        "train_images = {}\n",
        "train_profile_urls = {}\n",
        "\n",
        "for person_name, profiles in linkedin_data.items():\n",
        "    pics = []\n",
        "    profile_links = []\n",
        "    for p in profiles:\n",
        "        img_url = p.get(\"person_image_url\")\n",
        "        profile_url = p.get(\"person_profile_url\")\n",
        "        if isinstance(img_url, str) and img_url.startswith(\"https://media.licdn.com/dms/image/\" or \"https://www.capterra.com/\"):\n",
        "            pics.append(img_url)\n",
        "            profile_links.append(profile_url)\n",
        "    if pics:\n",
        "        train_images[person_name] = pics\n",
        "        train_profile_urls[person_name] = profile_links\n",
        "\n",
        "# Build test_images\n",
        "test_images = {}\n",
        "for entry in test_data:\n",
        "    name = entry.get(\"name\")\n",
        "    url = entry.get(\"image\") or entry.get(\"image_url\") or entry.get(\"person_image_url\")\n",
        "    if name and isinstance(url, str):\n",
        "        test_images[name] = [url]\n",
        "\n",
        "# Find intersection\n",
        "common = set(train_images) & set(test_images)\n",
        "\n",
        "# Assemble DataFrame\n",
        "rows = []\n",
        "for name in sorted(common):\n",
        "    rows.append({\n",
        "        \"name\": name,\n",
        "        \"train_imgs\": train_images[name],\n",
        "        \"train_profile_urls\": train_profile_urls[name],\n",
        "        \"actual_imgs\": test_images[name]\n",
        "    })\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import re\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "\n",
        "os.makedirs(\"train_images\", exist_ok=True)\n",
        "os.makedirs(\"actual_images\", exist_ok=True)\n",
        "\n",
        "# Converts Google Drive links to direct download\n",
        "def convert_drive_to_direct(url):\n",
        "    match = re.search(r\"/d/([a-zA-Z0-9_-]+)\", url)\n",
        "    if match:\n",
        "        file_id = match.group(1)\n",
        "        return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n",
        "    return url\n",
        "\n",
        "# Extracts real image URL if it's wrapped in a _next/image proxy\n",
        "def extract_direct_image_url(url):\n",
        "    if \"_next/image\" in url and \"url=\" in url:\n",
        "        parsed = urlparse(url)\n",
        "        query_params = parse_qs(parsed.query)\n",
        "        if 'url' in query_params:\n",
        "            return unquote(query_params['url'][0])\n",
        "    return url\n",
        "\n",
        "# Downloads and saves images\n",
        "def download_and_save_images(image_urls, folder, name_prefix):\n",
        "    for i, url in enumerate(image_urls):\n",
        "        try:\n",
        "            direct_url = extract_direct_image_url(convert_drive_to_direct(url))\n",
        "            response = requests.get(direct_url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "            img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "            img.save(os.path.join(folder, f\"{name_prefix}_{i}.jpg\"), \"JPEG\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "# Loop through each row and download train/actual images\n",
        "for idx, row in df.iterrows():\n",
        "    name = row['name'].replace(\" \", \"_\")\n",
        "    if isinstance(row['train_imgs'], list):\n",
        "        download_and_save_images(row['train_imgs'], \"train_images\", f\"{name}_train\")\n",
        "    if isinstance(row['actual_imgs'], list):\n",
        "        download_and_save_images(row['actual_imgs'], \"actual_images\", f\"{name}_actual\")\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from deepface import DeepFace\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "def get_embedding(img_path, model_name='VGG-Face'):\n",
        "    try:\n",
        "        embedding_obj = DeepFace.represent(img_path=img_path, model_name=model_name, enforce_detection=False)[0]\n",
        "        return embedding_obj['embedding']\n",
        "    except Exception as e:\n",
        "        print(f\"Error on {img_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def cosine_sim(emb1, emb2):\n",
        "    return cosine_similarity([emb1], [emb2])[0][0]\n",
        "\n",
        "names = [f.split('_actual_')[0] for f in os.listdir(\"actual_images\") if f.endswith('.jpg')]\n",
        "results = {}\n",
        "\n",
        "for name in names:\n",
        "    actual_path = f\"actual_images/{name}_actual_0.jpg\"\n",
        "    if not os.path.exists(actual_path):\n",
        "        continue\n",
        "    actual_emb = get_embedding(actual_path)\n",
        "    if actual_emb is None:\n",
        "        continue\n",
        "\n",
        "    max_score = -1\n",
        "    best_train_path = None\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        train_path = f\"train_images/{name}_train_{i}.jpg\"\n",
        "        if not os.path.exists(train_path):\n",
        "            break\n",
        "        train_emb = get_embedding(train_path)\n",
        "        if train_emb is not None:\n",
        "            score = cosine_sim(train_emb, actual_emb)\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "                best_train_path = train_path\n",
        "                best_index = i\n",
        "        i += 1\n",
        "\n",
        "    if max_score >= 0.20:\n",
        "        # Extract clean name and index\n",
        "        filename = os.path.basename(best_train_path)\n",
        "        parts = filename.split('_train_')\n",
        "        person_clean = parts[0].replace('_', ' ')\n",
        "        index = int(parts[1].split('.')[0])\n",
        "\n",
        "        # Fetch profile URL\n",
        "        try:\n",
        "            urls_array = np.array(df[df['name'] == person_clean]['train_profile_urls'])[0]\n",
        "            person_url = urls_array[index]\n",
        "        except Exception as e:\n",
        "            print(f\"URL fetch failed for {person_clean} at index {index}: {e}\")\n",
        "            person_url = None\n",
        "\n",
        "        results[name] = {\n",
        "            \"similarity\": round(max_score, 4),\n",
        "            \"person_profile_url\": person_url\n",
        "        }\n",
        "    else:\n",
        "        results[name] = \"No similarity found\"\n",
        "\n",
        "# Save result to JSON\n",
        "with open(\"pipe3_face_similarity_results.json\", \"w\") as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to pipe3_face_similarity_results.json\")\n",
        "\n",
        "import json\n",
        "\n",
        "# Load JSON data\n",
        "with open(\"pipe3_face_similarity_results.json\") as f:\n",
        "    face_data = json.load(f)\n",
        "\n",
        "with open(\"pipe3_final_companysize_validation_output.json\") as f:\n",
        "    linkedin_data = json.load(f)\n",
        "\n",
        "# Process each person\n",
        "for person in linkedin_data:\n",
        "    name_key = person[\"person_name\"].replace(\" \", \"_\")\n",
        "    face_match = face_data.get(name_key)\n",
        "\n",
        "    if face_match and \"person_profile_url\" in face_match:\n",
        "        # Override URL + use similarity from face JSON\n",
        "        person[\"person_profile_url\"] = face_match[\"person_profile_url\"]\n",
        "        person[\"confidence_score\"] = float(face_match.get(\"similarity\"))/100\n",
        "    else:\n",
        "        # Keep original URL + use existing similarity from match JSON as confidence_score\n",
        "        person[\"confidence_score\"] = float(person.get(\"similarity\"))/100\n",
        "\n",
        "    # Remove 'match' and old 'similarity' keys if they exist\n",
        "    person.pop(\"match\", None)\n",
        "    person.pop(\"similarity\", None)\n",
        "\n",
        "# Save result\n",
        "with open(\"pipe3_final.json\", \"w\") as f:\n",
        "    json.dump(linkedin_data, f, indent=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "6GqD0fw28hPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "\n",
        "with open(\"pipe1_final.json\", \"r\") as f:\n",
        "    data1 = json.load(f)\n",
        "\n",
        "for item in data1:\n",
        "    results.append(item)\n",
        "\n",
        "with open(\"pipe2_final.json\", \"r\") as f:\n",
        "    data2 = json.load(f)\n",
        "\n",
        "for item in data2:\n",
        "    results.append(item)\n",
        "\n",
        "with open(\"pipe3_final.json\",\"r\") as f:\n",
        "  results.append(json.load(f))\n",
        "\n",
        "with open(\"results.json\",\"w\") as f:\n",
        "  json.dump(results,f,indent=4)"
      ],
      "metadata": {
        "id": "veOjsTSK8hM9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "id": "Ql_WIs8n8hKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pKsfH5J28hGl"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}